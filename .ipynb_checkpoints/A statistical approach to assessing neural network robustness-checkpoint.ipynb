{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "((train_x, train_y),(eval_x, eval_y)) = tf.keras.datasets.mnist.load_data()\n",
    "train_x = train_x/np.float32(255)\n",
    "eval_x = eval_x/np.float32(255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HParams(object):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.dict_ = kwargs\n",
    "        self.__dict__.update(self.dict_)\n",
    "\n",
    "    def update_config(self, in_string):\n",
    "        pairs = in_string.split(\",\")\n",
    "        pairs = [pair.split(\"=\") for pair in pairs]\n",
    "        for key, val in pairs:\n",
    "            self.dict_[key] = type(self.dict_[key])(val)\n",
    "        self.__dict__.update(self.dict_)\n",
    "        return self\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self.dict_[key]\n",
    "\n",
    "    def __setitem__(self, key, val):\n",
    "        self.dict_[key] = val\n",
    "        self.__dict__.update(self.dict_)\n",
    "\n",
    "\n",
    "def get_default_hparams():\n",
    "    return HParams(\n",
    "        image_dim = 28,\n",
    "        epoch_num = 50,\n",
    "        batch_size = 1000,\n",
    "        learning_rate = 1e-3,\n",
    "        next_batch = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_pool(inputs, filters=32, kernel_size=[5,5], pool_size=[2,2]):\n",
    "    conv = tf.layers.conv2d(\n",
    "        inputs=inputs,\n",
    "        filters=filters,\n",
    "        kernel_size=kernel_size,\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu)\n",
    "    pool = tf.layers.max_pooling2d(inputs=conv, pool_size=pool_size, strides=2)\n",
    "    return pool\n",
    "\n",
    "def dense_layer(inputs, reshape=(7,7,64), units=1024, drop_out=True, drop_out_rate=0.4, training=True):\n",
    "    inputs = tf.reshape(inputs, [-1, reshape[0]*reshape[1]*reshape[2]])\n",
    "    dense = tf.layers.dense(inputs=inputs, units=units, activation=tf.nn.relu)\n",
    "    dropout = tf.layers.dropout(inputs=dense, rate=drop_out_rate, training=training)\n",
    "    return dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model(inputs, training=True):\n",
    "    with tf.variable_scope(\"cnn_model\", reuse=tf.AUTO_REUSE):\n",
    "        inputs = tf.expand_dims(inputs, 3)\n",
    "        conv1 = conv_pool(inputs)\n",
    "        conv2 = conv_pool(conv1, filters=64)\n",
    "        dense = dense_layer(conv2, training=training)\n",
    "        logits = tf.layers.dense(inputs=dense, units=10)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, batch_num, batch_size=100):\n",
    "    if batch_size == 0:\n",
    "        raise ValueError(\"batch_size cannot be zero\")\n",
    "    elif batch_num < 0:\n",
    "        raise ValueError(\"batch_num must be larger than or equal to zero\")\n",
    "    elif data is np.ndarray:\n",
    "        raise TypeError(\"input data should be numpy.ndarray\")\n",
    "    elif data.shape[0] < batch_size:\n",
    "        raise TypeError(\"please reduce the batch_size less than the data number\")\n",
    "    number_of_batch = data.shape[0]//batch_size\n",
    "    if batch_num >= number_of_batch:\n",
    "        end_of_batch = True\n",
    "    else:\n",
    "        end_of_batch = False\n",
    "    batch_num = batch_num % number_of_batch\n",
    "    return data[batch_num*batch_size:(batch_num+1)*batch_size], end_of_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(train_x, train_y, hps, \n",
    "             restore=False, save=False, clean_graph=False, \n",
    "             sess=tf.Session(), path=\"./trained_model\", filename=\"model.ckpt\"):\n",
    "    inputs = tf.placeholder(tf.float32, shape=(None, hps.image_dim, hps.image_dim))\n",
    "    labels = tf.placeholder(tf.int32, shape=(None))\n",
    "    logits = cnn_model(inputs)\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=hps.learning_rate).minimize(loss)\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    sess.run([tf.global_variables_initializer()])\n",
    "    if restore:\n",
    "        saver.restore(sess, \"{0}/{1}\".format(path, filename))\n",
    "\n",
    "    for epoch in range(hps.epoch_num):\n",
    "        next_batch = hps.next_batch\n",
    "        while True:\n",
    "            train_data, end_of_batch = get_batch(train_x, batch_size=hps.batch_size, batch_num=next_batch)\n",
    "            train_labels, _ = get_batch(train_y, batch_size=hps.batch_size, batch_num=next_batch)\n",
    "            next_batch += 1\n",
    "            sess.run(optimizer, feed_dict={inputs: train_data, labels: train_labels})\n",
    "            loss_run_time = sess.run(loss, feed_dict={inputs: train_data, labels: train_labels})\n",
    "            if end_of_batch:\n",
    "                break\n",
    "        print(\"Epoch: {}, Loss:{}\".format(epoch, loss_run_time))\n",
    "    if save:\n",
    "        save_path = saver.save(sess, \"{0}/{1}\".format(path, filename))\n",
    "        print(\"Model is saved to {0}\".format(save_path))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(eval_x, eval_y, hps, \n",
    "               sess=tf.Session(),\n",
    "               path=\"./trained_model\", filename=\"model.ckpt\", print_output=True):\n",
    "    inputs = tf.placeholder(tf.float32, shape=(None, hps.image_dim, hps.image_dim))\n",
    "    labels = tf.placeholder(tf.int32, shape=(None))\n",
    "    logits = cnn_model(inputs, training=False)\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run([tf.global_variables_initializer()])\n",
    "    saver.restore(sess, \"./trained_model/model.ckpt\")\n",
    "    loss_run_time, logits_run_time = sess.run([loss, logits], feed_dict={inputs: eval_x, labels: eval_y})\n",
    "    accuracy = 100 * np.sum(np.argmax(logits_run_time, axis=1) == eval_y) / eval_y.shape[0]\n",
    "    if print_output:\n",
    "        print(\"Loss:{}, Accuracy:{}%\".format(loss_run_time, accuracy))\n",
    "    return logits_run_time, eval_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_model(hps, \n",
    "               sess=tf.Session(),\n",
    "               path=\"./trained_model\", \n",
    "               filename=\"model.ckpt\"):\n",
    "    inputs = tf.placeholder(tf.float32, shape=(None, hps.image_dim, hps.image_dim))\n",
    "    labels = tf.placeholder(tf.int32, shape=(None))\n",
    "    logits = cnn_model(inputs, training=False)\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run([tf.global_variables_initializer()])\n",
    "    saver.restore(sess, \"./trained_model/model.ckpt\")\n",
    "    return inputs, labels, logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_perturbation(sample, x_min, x_max, n=1, sigma=1, seed=0, image_size=28):\n",
    "    import torch\n",
    "    import torch.distributions as dist\n",
    "    sample = torch.tensor(sample).view(-1, image_size*image_size)\n",
    "    if isinstance(x_min, (int, float, complex)) and isinstance(x_max, (int, float, complex)):\n",
    "        prior = dist.Uniform(low=torch.max(sample-sigma, torch.tensor([x_min])), high=torch.min(sample+sigma, torch.tensor([x_max])))\n",
    "    elif isinstance(x_min, torch.Tensor) and isinstance(x_max, torch.Tensor):\n",
    "        prior = dist.Uniform(low=torch.max(sample-sigma, x_min), high=torch.min(sample+sigma, x_max))\n",
    "    else:\n",
    "        raise ValueError('Type of x_min and x_max {0} is not supported'.format(type(x_min)))\n",
    "    x = prior.sample(torch.Size([n])).view(-1, image_size, image_size)\n",
    "    return x.numpy(), prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_grey_scale_numpy_array(array):\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline  \n",
    "    imgplot = plt.imshow(array, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_property_function(logits, labels):\n",
    "    equal_to_labels = logits.argmax(axis=1) == labels\n",
    "    correct_logits = np.array([logits[idx][label] for idx, label in enumerate(labels)]) \n",
    "    z_c = np.exp(correct_logits)/np.exp(logits).sum(axis=1)\n",
    "    logits_rankings = np.argsort(logits, axis=1)\n",
    "    best_logits_without_labels = np.array([logit[logits_ranking[-2]] if equal_to_label else logit[logits_ranking[-1]] \n",
    "                                           for logit, logits_ranking, equal_to_label in zip(logits, logits_rankings, equal_to_labels)])\n",
    "    z_i = np.exp(best_logits_without_labels)/np.exp(logits).sum(axis=1)\n",
    "    return z_i - z_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The bounds in NN-space\n",
    "x_min = (0-0.1307)/0.3081\n",
    "x_max = (1-0.1307)/0.3081"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_s_xn(samples, input_labels):\n",
    "    tf.reset_default_graph()\n",
    "    with tf.Session() as sess:\n",
    "        inputs, labels, logits, loss = evaluation_model(get_default_hparams(), sess=sess)\n",
    "        loss_run_time, logits_run_time = sess.run([loss, logits], feed_dict={inputs: samples, labels: input_labels})\n",
    "        accuracy = 100 * np.sum(np.argmax(logits_run_time, axis=1) == perturbated_labels) / perturbated_labels.shape[0]\n",
    "        s_xn = compute_property_function(logits_run_time, perturbated_labels)\n",
    "    return s_xn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./trained_model/model.ckpt\n",
      "Loss:0.025718383491039276, Accuracy:99.35%\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    logits, labels = evaluation(eval_x, eval_y, get_default_hparams(), sess=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_idx = random.randrange(0, eval_x.shape[0])\n",
    "sample_x, sample_y = eval_x[sample_idx], eval_y[sample_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_grey_scale_numpy_array(sample_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Naive MCMC</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "tf.reset_default_graph()\n",
    "accuracy_list = []\n",
    "step = 0\n",
    "sample_size = 10000\n",
    "with tf.Session() as sess:\n",
    "    inputs, labels, logits, loss = evaluation_model(get_default_hparams(), sess=sess)\n",
    "    for i in range(500):\n",
    "        perturbated_samples, _ = uniform_perturbation(sample_x, x_min, x_max, n=sample_size, sigma=sigma)\n",
    "        perturbated_labels = np.repeat(sample_y, sample_size)\n",
    "        loss_run_time, logits_run_time = sess.run([loss, logits], feed_dict={inputs: perturbated_samples, labels: perturbated_labels})\n",
    "        accuracy = 100 * np.sum(np.argmax(logits_run_time, axis=1) == perturbated_labels) / perturbated_labels.shape[0]\n",
    "        accuracy_list += [1-accuracy/100]\n",
    "end_time = time.time()\n",
    "log_i = math.log(np.array(accuracy_list).mean())\n",
    "run_time = end_time - start_time\n",
    "print(\"Run time: {0} | log_i: {1}\".format(run_time, log_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress = [math.log(np.array(accuracy_list[:i]).mean()) for i in range(1, len(accuracy_list))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Adaptive Multi-level Splitting</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mh_update(samples, sample_labels, width_proposal, l_k, inputs, labels, \n",
    "              width_inc=1.02, width_dec=0.5, update_steps=250, sess=tf.Session()):\n",
    "    #Take reference to the original code from the paper, which provides a efficient method for MH process\n",
    "    import torch\n",
    "    import torch.distributions as dist\n",
    "    sample_size = samples.shape[0]\n",
    "    x = torch.tensor(samples).view(-1, 28*28)\n",
    "    acc_ratio = torch.zeros(sample_size)\n",
    "    for i in range(update_steps):\n",
    "        g_bottom = dist.Uniform(low=torch.max(x - width_proposal.unsqueeze(-1), prior.low), high=torch.min(x + width_proposal.unsqueeze(-1), prior.high))\n",
    "        x_new = g_bottom.sample()\n",
    "        loss_run_time, logits_run_time = sess.run([loss, logits], feed_dict={inputs: x_new.view(-1, 28, 28).numpy(), \n",
    "                                                                             labels: sample_labels})\n",
    "        s_xn = compute_property_function(logits_run_time, sample_labels)\n",
    "        \n",
    "        g_top = dist.Uniform(low=torch.max(x_new - width_proposal.unsqueeze(-1), prior.low), high=torch.min(x_new + width_proposal.unsqueeze(-1), prior.high))\n",
    "        lg_alpha = (prior.log_prob(x_new) - prior.log_prob(x)+ g_top.log_prob(x) - g_bottom.log_prob(x_new)).sum(dim=1)\n",
    "        acceptance = torch.min(lg_alpha, torch.zeros_like(lg_alpha))\n",
    "        \n",
    "        log_u = torch.log(torch.rand_like(acceptance))\n",
    "        acc_idx = torch.tensor((log_u <= acceptance).numpy() & (s_xn >= l_k))\n",
    "        acc_ratio += acc_idx.float()\n",
    "        x = torch.where(acc_idx.unsqueeze(-1), x_new, x)\n",
    "    if not all(s_x >= l_k for s_x in s_xn):\n",
    "        print(acc_idx)\n",
    "        print(s_xn)\n",
    "        raise ValueError(\"Bug!\")\n",
    "    width_proposal = torch.where(acc_ratio > 0.124, width_proposal*width_inc, width_proposal)\n",
    "    width_proposal = torch.where(acc_ratio < 0.124, width_proposal*width_dec, width_proposal)  \n",
    "    return x.view(-1, 28, 28), width_proposal\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 100\n",
    "perturbated_samples, prior = uniform_perturbation(sample_x, x_min, x_max, n=sample_size, sigma=sigma)\n",
    "perturbated_labels = np.repeat(sample_y, sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile = 0.1\n",
    "log_p_min = -250\n",
    "l_k = float('-inf')\n",
    "l_prev = float('-inf')\n",
    "log_i = 0\n",
    "k = 0\n",
    "width_proposal = sigma*torch.ones(sample_size)/30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "width_proposal = sigma*torch.ones(sample_size)/30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mh_update(samples, sample_labels, width_proposal, l_k, inputs, labels, \n",
    "              width_inc=1.02, width_dec=0.5, update_steps=250, sess=tf.Session()):\n",
    "    #Take reference to the original code from the paper, which provides a efficient method for MH process\n",
    "    import torch\n",
    "    import torch.distributions as dist\n",
    "    sample_size = samples.shape[0]\n",
    "    x = torch.tensor(samples).view(-1, 28*28)\n",
    "    acc_ratio = torch.zeros(sample_size)\n",
    "    for i in range(update_steps):\n",
    "        g_bottom = dist.Uniform(low=torch.max(x - width_proposal.unsqueeze(-1), prior.low), high=torch.min(x + width_proposal.unsqueeze(-1), prior.high))\n",
    "        x_new = g_bottom.sample()\n",
    "        loss_run_time, logits_run_time = sess.run([loss, logits], feed_dict={inputs: x_new.view(-1, 28, 28).numpy(), \n",
    "                                                                             labels: sample_labels})\n",
    "        s_xn = compute_property_function(logits_run_time, sample_labels)\n",
    "        \n",
    "        g_top = dist.Uniform(low=torch.max(x_new - width_proposal.unsqueeze(-1), prior.low), high=torch.min(x_new + width_proposal.unsqueeze(-1), prior.high))\n",
    "        lg_alpha = (prior.log_prob(x_new) - prior.log_prob(x)+ g_top.log_prob(x) - g_bottom.log_prob(x_new)).sum(dim=1)\n",
    "        acceptance = torch.min(lg_alpha, torch.zeros_like(lg_alpha))\n",
    "        \n",
    "        log_u = torch.log(torch.rand_like(acceptance))\n",
    "        acc_idx = torch.tensor((log_u <= acceptance).numpy() & (s_xn >= l_k))\n",
    "        acc_ratio += acc_idx.float()\n",
    "        x = torch.where(acc_idx.unsqueeze(-1), x_new, x)\n",
    "    if not all(s_x >= l_k for s_x in s_xn):\n",
    "        print(acc_idx)\n",
    "        print(s_xn)\n",
    "        raise ValueError(\"Bug!\")\n",
    "    width_proposal = torch.where(acc_ratio > 0.124, width_proposal*width_inc, width_proposal)\n",
    "    width_proposal = torch.where(acc_ratio < 0.124, width_proposal*width_dec, width_proposal)  \n",
    "    return x.view(-1, 28, 28), width_proposal\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./trained_model/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "log_i_record = []\n",
    "step = 0\n",
    "with tf.Session() as sess:\n",
    "    inputs, labels, logits, loss = evaluation_model(get_default_hparams(), sess=sess)\n",
    "    while l_k < 0:\n",
    "        loss_run_time, logits_run_time = sess.run([loss, logits], feed_dict={inputs: perturbated_samples, labels: perturbated_labels})\n",
    "        accuracy = 100 * np.sum(np.argmax(logits_run_time, axis=1) == perturbated_labels) / perturbated_labels.shape[0]\n",
    "        s_xn = compute_property_function(logits_run_time, perturbated_labels)\n",
    "        \n",
    "        l_k = min(0, np.quantile(s_xn, quantile, interpolation='lower'))\n",
    "        p_k = np.where(s_xn>= l_k)[0].shape[0] / sample_size\n",
    "        log_i += math.log(p_k)\n",
    "        if log_i < log_p_min:\n",
    "            break\n",
    "        s_xn = s_xn[s_xn>=l_k]\n",
    "        print(len(s_xn))\n",
    "        resample_idx = np.random.choice(s_xn.shape[0], sample_size)\n",
    "        perturbated_samples = perturbated_samples[resample_idx]\n",
    "        s_xn = s_xn[resample_idx]\n",
    "        if not all(s_x >= l_k for s_x in s_xn):\n",
    "            raise ValueError(\"Bug Ar!\")\n",
    "        #perturbated_samples, width_proposal = mh_update(perturbated_samples, perturbated_labels, width_proposal, l_k, sess=sess, inputs=inputs, labels=labels)\n",
    "        print(\"Loss:{}, Accuracy:{}%\".format(loss_run_time, accuracy))\n",
    "        print(\"log_i: {0} | l_k: {1}\".format(log_i, l_k))\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:3.6640048027038574, Accuracy:9.0%\n",
      "log_i: -2.9257125365219485 | l_k: 0\n"
     ]
    }
   ],
   "source": [
    "    print(\"Loss:{}, Accuracy:{}%\".format(loss_run_time, accuracy))\n",
    "    print(\"log_i: {0} | l_k: {1}\".format(log_i, l_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######NAIVE MCMC##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./trained_model/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "tf.reset_default_graph()\n",
    "accuracy_list = []\n",
    "step = 0\n",
    "sample_size = 10000\n",
    "with tf.Session() as sess:\n",
    "    inputs, labels, logits, loss = evaluation_model(get_default_hparams(), sess=sess)\n",
    "    for i in range(500):\n",
    "        perturbated_samples, _ = uniform_perturbation(sample_x, x_min, x_max, n=sample_size, sigma=1)\n",
    "        perturbated_labels = np.repeat(sample_y, sample_size)\n",
    "        loss_run_time, logits_run_time = sess.run([loss, logits], feed_dict={inputs: perturbated_samples, labels: perturbated_labels})\n",
    "        accuracy = 100 * np.sum(np.argmax(logits_run_time, axis=1) == perturbated_labels) / perturbated_labels.shape[0]\n",
    "        accuracy_list += [1-accuracy/100]\n",
    "end_time = time.time()\n",
    "log_i = math.log(np.array(accuracy_list).mean())\n",
    "run_time = end_time - start_time\n",
    "print(\"Run time: {0} | log_i: {1}\".format(run_time, log_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(accuracy_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deep_strive/anaconda3/envs/florence/lib/python3.6/site-packages/ipykernel_launcher.py:1: RuntimeWarning: Mean of empty slice.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/deep_strive/anaconda3/envs/florence/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "progress = [math.log(np.array(accuracy_list[:i]).mean()) for i in range(1, len(accuracy_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[nan,\n",
       " -0.17078832098028163,\n",
       " -0.17447244185055563,\n",
       " -0.17177734291828353,\n",
       " -0.17242073211669215,\n",
       " -0.17342524657195574,\n",
       " -0.17344110498801746,\n",
       " -0.17414932633394822,\n",
       " -0.17317848237767447,\n",
       " -0.17379798585307782,\n",
       " -0.17299716437363755,\n",
       " -0.17283938231356255,\n",
       " -0.17310416774434573,\n",
       " -0.1724412969410177,\n",
       " -0.17229767843330662,\n",
       " -0.1716744390501945,\n",
       " -0.17123325980853418,\n",
       " -0.1714444569216786,\n",
       " -0.1714277767395384,\n",
       " -0.17125043685469174,\n",
       " -0.1714765767537925,\n",
       " -0.17181692414886915,\n",
       " -0.1719104860131216,\n",
       " -0.17138677739337196,\n",
       " -0.1712431495869537,\n",
       " -0.17110628374335174,\n",
       " -0.1710347241792942,\n",
       " -0.17076196044727007,\n",
       " -0.1708137407237977,\n",
       " -0.17073105587966342,\n",
       " -0.17078832098028163,\n",
       " -0.17081893408422327,\n",
       " -0.17082539165552812,\n",
       " -0.17082426828154174,\n",
       " -0.17079180992642787,\n",
       " -0.17081543539634028,\n",
       " -0.1707125363207077,\n",
       " -0.17065367581816726,\n",
       " -0.17063849141113585,\n",
       " -0.1705754288397594,\n",
       " -0.17055703100389177,\n",
       " -0.17049903609050673,\n",
       " -0.17055674863124431,\n",
       " -0.17044905883082215,\n",
       " -0.17048641465520742,\n",
       " -0.1705511011950391,\n",
       " -0.17065423331769278,\n",
       " -0.17058390483372265,\n",
       " -0.17042016021094183,\n",
       " -0.17051963749144422,\n",
       " -0.17052975417431182,\n",
       " -0.17057435554883693,\n",
       " -0.17054197848084943,\n",
       " -0.170490686278802,\n",
       " -0.1703885941948025,\n",
       " -0.17049719545460412,\n",
       " -0.17048756973529774,\n",
       " -0.1705947953484563,\n",
       " -0.1705613250253564,\n",
       " -0.17055914162230754,\n",
       " -0.17057877393719237,\n",
       " -0.17061720597113342,\n",
       " -0.1706467475637065,\n",
       " -0.17067911758131146,\n",
       " -0.1707123303919573,\n",
       " -0.17067152882274203,\n",
       " -0.1707092414657884,\n",
       " -0.17072635515883816,\n",
       " -0.17065226157444185,\n",
       " -0.17045657275866666,\n",
       " -0.17048502856085998,\n",
       " -0.17047259718085464,\n",
       " -0.17054286540774302,\n",
       " -0.1706843272978039,\n",
       " -0.1706472645959639,\n",
       " -0.17071398606031152,\n",
       " -0.170655658243879,\n",
       " -0.17065275992676449,\n",
       " -0.17064537411521258,\n",
       " -0.17068772090827516,\n",
       " -0.17074680345543172,\n",
       " -0.17073852944571083,\n",
       " -0.17075794213437154,\n",
       " -0.17069685608415897,\n",
       " -0.1706231083957903,\n",
       " -0.1706501684973989,\n",
       " -0.17068625440796079,\n",
       " -0.17072014861324245,\n",
       " -0.17074922975686269,\n",
       " -0.17070702022140738,\n",
       " -0.17058799835475275,\n",
       " -0.17058889618329787,\n",
       " -0.17059622018995024,\n",
       " -0.1706263422374053,\n",
       " -0.17067096586166539,\n",
       " -0.17065721887965069,\n",
       " -0.1706338746238275,\n",
       " -0.17070394254662535,\n",
       " -0.170669704053507,\n",
       " -0.17066850597450692]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16.1]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all(s_x > l_k for s_x in s_xn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributions as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = perturbated_samples\n",
    "sample_labels = perturbated_labels\n",
    "width_inc=1.02\n",
    "width_dec=0.5\n",
    "update_steps=250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = samples.shape[0]\n",
    "x = torch.tensor(samples).view(-1, 28*28)\n",
    "acc_ratio = torch.zeros(sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./trained_model/model.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    inputs, labels, logits, loss = evaluation_model(get_default_hparams(), sess=sess)\n",
    "    loss_run_time, logits_run_time = sess.run([loss, logits], feed_dict={inputs: perturbated_samples, labels: perturbated_labels})\n",
    "    accuracy = 100 * np.sum(np.argmax(logits_run_time, axis=1) == perturbated_labels) / perturbated_labels.shape[0]\n",
    "    s_xn = compute_property_function(logits_run_time, perturbated_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./trained_model/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    inputs, labels, logits, loss = evaluation_model(get_default_hparams(), sess=sess)\n",
    "    g_bottom = dist.Uniform(low=torch.max(x - width_proposal.unsqueeze(-1), prior.low), high=torch.min(x + width_proposal.unsqueeze(-1), prior.high))\n",
    "    x_new = g_bottom.sample()\n",
    "    loss_run_time, logits_run_time = sess.run([loss, logits], feed_dict={inputs: x_new.view(-1, 28, 28).numpy(), \n",
    "                                                                         labels: sample_labels})\n",
    "    s_xn = compute_property_function(logits_run_time, sample_labels)\n",
    "\n",
    "    g_top = dist.Uniform(low=torch.max(x_new - width_proposal.unsqueeze(-1), prior.low), high=torch.min(x_new + width_proposal.unsqueeze(-1), prior.high))\n",
    "    lg_alpha = (prior.log_prob(x_new) - prior.log_prob(x)+ g_top.log_prob(x) - g_bottom.log_prob(x_new)).sum(dim=1)\n",
    "    acceptance = torch.min(lg_alpha, torch.zeros_like(lg_alpha))\n",
    "\n",
    "    log_u = torch.log(torch.rand_like(acceptance))\n",
    "    acc_idx = torch.tensor((log_u <= acceptance).numpy() & (s_xn >= l_k))\n",
    "    acc_ratio += acc_idx.float()\n",
    "    x = torch.where(acc_idx.unsqueeze(-1), x_new, x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1], dtype=torch.uint8)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./trained_model/model.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.9971117, -0.9971117, -0.9971117, -0.9971117, -0.9971117,\n",
       "       -0.9971117, -0.9971117, -0.9971117, -0.9971117, -0.9971117,\n",
       "       -0.9971117, -0.9971117, -0.9971117, -0.9971117, -0.9971117,\n",
       "       -0.9971117, -0.9971117, -0.9971117, -0.9971117, -0.9971117,\n",
       "       -0.9971117, -0.9971117, -0.9971117, -0.9971117, -0.9971117,\n",
       "       -0.9971117, -0.9971117, -0.9971117, -0.9971117, -0.9971117,\n",
       "       -0.9971117, -0.9971117, -0.9971117, -0.9971117, -0.9971117,\n",
       "       -0.9971117, -0.9971117, -0.9971117, -0.9971117, -0.9971117,\n",
       "       -0.9971117, -0.9971117, -0.9971117, -0.9971117, -0.9971117,\n",
       "       -0.9971117, -0.9971117, -0.9971117, -0.9971117, -0.9971117,\n",
       "       -0.9971117, -0.9971117, -0.9971117, -0.9971117, -0.9971117,\n",
       "       -0.9971117, -0.9971117, -0.9971117, -0.9971117, -0.9971117,\n",
       "       -0.9971117, -0.9971117, -0.9971117, -0.9971117, -0.9971117,\n",
       "       -0.9971117, -0.9971117, -0.9971117, -0.9971117, -0.9971117,\n",
       "       -0.9971117, -0.9971117, -0.9971117, -0.9971117, -0.9971117,\n",
       "       -0.9971117, -0.9971117, -0.9971117, -0.9971117, -0.9971117,\n",
       "       -0.9971117, -0.9971117, -0.9971117, -0.9971117, -0.9971117,\n",
       "       -0.9971117, -0.9971117, -0.9971117, -0.9971117, -0.9971117,\n",
       "       -0.9971117, -0.9971117, -0.9971117, -0.9971117, -0.9971117,\n",
       "       -0.9971117, -0.9971117, -0.9971117, -0.9971117, -0.9971117],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_s_xn(perturbated_samples, perturbated_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0], dtype=torch.uint8)\n",
      "[-0.9958999  -0.9965055  -0.9972348  -0.9974737  -0.99704885 -0.9976496\n",
      " -0.9968525  -0.9966381  -0.9964348  -0.9974204  -0.9972643  -0.9958302\n",
      " -0.99673784 -0.99641526 -0.9962485  -0.9975514  -0.99747473 -0.99624944\n",
      " -0.9959975  -0.9961274  -0.9974468  -0.99725074 -0.9968761  -0.99654233\n",
      " -0.9966587  -0.99713135 -0.99712294 -0.9971644  -0.99609673 -0.9970925\n",
      " -0.9961018  -0.9967678  -0.9960247  -0.99743515 -0.9969177  -0.99675685\n",
      " -0.99811584 -0.99716586 -0.9968919  -0.9964589  -0.99601    -0.99644595\n",
      " -0.9966746  -0.99623936 -0.996497   -0.99662864 -0.9959126  -0.99777704\n",
      " -0.9965712  -0.99616694 -0.99601716 -0.9972186  -0.99525523 -0.9976316\n",
      " -0.99647725 -0.9969916  -0.997802   -0.9975264  -0.997624   -0.9978108\n",
      " -0.9976131  -0.9960984  -0.9969278  -0.99667895 -0.9958547  -0.99694836\n",
      " -0.9960881  -0.9965563  -0.996195   -0.9968867  -0.9974724  -0.9962626\n",
      " -0.9963294  -0.9971812  -0.9974462  -0.9977808  -0.99563026 -0.9971063\n",
      " -0.9971495  -0.9960839  -0.996072   -0.99653447 -0.99588025 -0.995082\n",
      " -0.9966721  -0.9969769  -0.9976759  -0.9967691  -0.9966782  -0.9961338\n",
      " -0.996972   -0.9973022  -0.9969551  -0.9967263  -0.99697953 -0.9962397\n",
      " -0.99646884 -0.9971138  -0.99688584 -0.99465597]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Bug!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-9597015823a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_xn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Bug!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: Bug!"
     ]
    }
   ],
   "source": [
    "if not all(s_x >= l_k for s_x in s_xn):\n",
    "    print(acc_idx)\n",
    "    print(s_xn)\n",
    "    raise ValueError(\"Bug!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_list += [3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resample_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    inputs, labels, logits, loss = evaluation_model(get_default_hparams(), sess=sess)\n",
    "    logits_run_time = sess.run(logits, feed_dict={inputs: np.array([sample_x])})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([sample_x]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_run_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logits_run_time.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_grey_scale_numpy_array(perturbated_samples[55])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_bottom = dist.Uniform(low=torch.max(x - width_proposal.unsqueeze(-1), prior.low), high=torch.min(x + width_proposal.unsqueeze(-1), prior.high))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbated_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_possible = uniform_perturbation(perturbated_samples, prior_low, prior_high, n=1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_possible.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resample_idx = np.random.choice(s_xn.shape[0], sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resample_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(s_xn>= l)[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_x = compute_property_function(logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "math.log(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_grey_scale_numpy_array(perturbated_samples[55])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Below are Testing </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_quantile = 0.1\n",
    "termination_threshold = math.exp(-250)\n",
    "MH_steps = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_perturbation(inputs, mean=0, stddev=0.3, seed=0):\n",
    "    perturbation = tf.random.normal(shape,mean=0.0,stddev=1.0,dtype=tf.dtypes.float32,seed=None,name=\"pertubation\")\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_perturbation(data, mean=0, sd=0.3):\n",
    "    perturbation = np.random.normal(loc=mean, scale=sd, size=data.shape)\n",
    "    return data+perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_perturbation(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proposal(x_prior=0):\n",
    "    return scipy.stats.norm(x_prior,1).rvs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_density(x):\n",
    "    if x > 1 or x < 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proposal_ratio(x, x_prior):\n",
    "    x_prior_against_x = scipy.stats.norm(x,1).pdf(x_prior)\n",
    "    x_against_x_prior = scipy.stats.norm(x_prior,1).pdf(x)\n",
    "    return x_prior_against_x/x_against_x_prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_density_ratio(x, x_prior):\n",
    "    if x_prior < 0 or x_prior > 1:\n",
    "        return 1e10\n",
    "    else:\n",
    "        return real_density(x)/real_density(x_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 100000\n",
    "x = proposal(3)\n",
    "result = []\n",
    "for i in range(M):\n",
    "    result += [x]\n",
    "    x_new = proposal(x)\n",
    "    alpha = min(1, proposal_ratio(x_new, x)*real_density_ratio(x_new, x))\n",
    "    u = random.random()\n",
    "    if u < alpha:\n",
    "        x = x_new\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "plt.hist(result[10000:], bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (flo)",
   "language": "python",
   "name": "florence"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
